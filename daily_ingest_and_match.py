# -*- coding: utf-8 -*-
"""daily_ingest_and_match

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nroock83FulGQHd1cKlOUDWhZ1MQ_2fL
"""

# Installing important libraries & Importing from them.
!apt-get -y install postgresql postgresql-contrib > /dev/null
!service postgresql start
!sudo -u postgres psql -c "ALTER USER postgres WITH PASSWORD 'mypassword';"
!pg_isready
!pip install rapidfuzz
!pip install --upgrade pip
!pip install rapidfuzz==3.6.1
import re
import pandas as pd
from rapidfuzz import fuzz
import random
import numpy as np
from sqlalchemy import create_engine, text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
!pip install flask pyngrok --quiet
from flask import Flask, render_template_string, request, redirect, url_for
from pyngrok import ngrok



"""
Retail Item Master Project
File: daily_ingest_and_match.py

In this script I automated the daily ingestion and matching flow for new items.
I assumed that:
- The central_item_master, itemmapping, and review_queue tables already exist
  (built by my main pipeline).
- A new POS items file is dropped daily in a known location.

For each run I:
1. Connected to PostgreSQL.
2. Read the daily new items CSV.
3. Inserted the new items into the rawitems table.
4. Called my matching engine (process_new_item) for each item so that it is
   either auto-mapped to central_item_master or sent to review_queue.
"""

import os
import pandas as pd
from sqlalchemy import create_engine, text

from matching_engine import process_new_item


# ============================================================
# 1. I configured the database connection and input file path
# ============================================================

def get_engine():
    """
    I created a SQLAlchemy engine for PostgreSQL.
    In a real environment I would move the connection string to an environment variable.
    """
    return create_engine(
        "postgresql+psycopg2://postgres:mypassword@localhost:5432/postgres"
    )


def get_daily_file_path():
    """
    I determined the path of the daily new items file.
    Here I used an environment variable so that the location can be configured
    without changing the code.

    Example:
      export DAILY_NEW_ITEMS_FILE="/data/incoming/2025-12-01_new_items.csv"
    """
    path = os.environ.get("DAILY_NEW_ITEMS_FILE")
    if not path:
        raise ValueError(
            "DAILY_NEW_ITEMS_FILE environment variable is not set. "
            "I expected it to point to the new items CSV for this run."
        )
    return path


# ============================================================
# 2. I loaded the new items from the daily CSV
# ============================================================

def load_new_items_from_csv(file_path: str) -> pd.DataFrame:
    """
    I read the daily CSV that contains new or updated items from stores.
    I expected at least these columns:
        Store_Name, Store_Item_ID, POSCode, ItemName, Brand, PackSize, Category
    """
    df = pd.read_csv(file_path)

    required_cols = [
        "Store_Name",
        "Store_Item_ID",
        "POSCode",
        "ItemName",
        "Brand",
        "PackSize",
        "Category",
    ]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(
            f"Daily file is missing required columns: {missing}. "
            "I expected at least: " + ", ".join(required_cols)
        )

    return df[required_cols]


# ============================================================
# 3. I inserted new items into the rawitems table
# ============================================================

def append_to_rawitems(df_new: pd.DataFrame, engine) -> None:
    """
    I appended the daily new items into the rawitems table so that
    the raw layer always contains a full history of what stores sent.
    """
    # I ensured the rawitems table exists with the right structure.
    ddl = """
    CREATE TABLE IF NOT EXISTS rawitems (
        "Store_Name"    VARCHAR(20),
        "Store_Item_ID" INT,
        "POSCode"       VARCHAR(50),
        "ItemName"      VARCHAR(255),
        "Brand"         VARCHAR(100),
        "PackSize"      VARCHAR(50),
        "Category"      VARCHAR(100)
    );
    """
    with engine.begin() as conn:
        conn.execute(text(ddl))

    # I appended today's new items to rawitems.
    df_new.to_sql("rawitems", engine, if_exists="append", index=False)


# ============================================================
# 4. I called the matching engine for each new item
# ============================================================

def match_new_items(df_new: pd.DataFrame, engine) -> None:
    """
    I looped through each new item and passed it to process_new_item.
    The matching logic inside process_new_item decides whether to:
      - Auto-link the item to an existing master (itemmapping), or
      - Send the item to review_queue for manual steward review.
    """
    for _, row in df_new.iterrows():
        new_item = {
            "Store_Name": row["Store_Name"],
            "Store_Item_ID": int(row["Store_Item_ID"]),
            "POSCode": row.get("POSCode"),
            "ItemName": row["ItemName"],
            "Brand": row["Brand"],
            "PackSize": row["PackSize"],
            "Category": row["Category"],
        }
        # I delegated the actual decision to my matching engine.
        process_new_item(new_item, engine)


# ============================================================
# 5. I wired everything together in a main function
# ============================================================

def main():
    """
    I implemented the end-to-end daily flow:
    - Read the daily file path from an environment variable.
    - Load new items.
    - Append them to rawitems.
    - Run matching for each new item.
    """
    engine = get_engine()
    daily_file = get_daily_file_path()

    df_new = load_new_items_from_csv(daily_file)
    append_to_rawitems(df_new, engine)
    match_new_items(df_new, engine)


if __name__ == "__main__":
    # I called main so that this script can be scheduled by cron or an orchestrator.
    main()