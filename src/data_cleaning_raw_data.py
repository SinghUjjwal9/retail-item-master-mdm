# -*- coding: utf-8 -*-
"""data_cleaning_raw_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vcSH2j1J5K7YJHK00EDTYALH1BAxNsn8
"""

# Installing important libraries & Importing from them.
!pip install rapidfuzz
!pip install --upgrade pip
!pip install rapidfuzz==3.6.1
import re
import pandas as pd
from rapidfuzz import fuzz
import random
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
!pip install flask pyngrok --quiet
from flask import Flask, render_template_string, request, redirect, url_for
from pyngrok import ngrok


# --- Install PostgreSQL & Python drivers ---
!apt-get -y update
!apt-get -y install postgresql postgresql-contrib
!pip install sqlalchemy psycopg[binary]

# --- Start PostgreSQL service ---
!service postgresql start

# --- Configure PostgreSQL (create user + DB) ---
# Default postgres user has no password; set your own here
PG_USER = "colab_user"
PG_PASS = "colab_pass"
PG_DB   = "colab_db"

# Create user and database
!sudo -u postgres psql -c "DROP DATABASE IF EXISTS {PG_DB};"
!sudo -u postgres psql -c "DROP ROLE IF EXISTS {PG_USER};"
!sudo -u postgres psql -c "CREATE ROLE {PG_USER} WITH LOGIN PASSWORD '{PG_PASS}';"
!sudo -u postgres psql -c "CREATE DATABASE {PG_DB} OWNER {PG_USER};"

# --- SQLAlchemy engine setup ---
from sqlalchemy import create_engine, text

engine = create_engine(
    f"postgresql+psycopg://{PG_USER}:{PG_PASS}@localhost:5432/{PG_DB}",
    echo=True
)
# ============================================================
# 1. Create and populate the rawitems table in PostgreSQL
# ============================================================

def create_rawitems_table(engine):
    """
    Create the rawitems table to store the original records exactly
    as they came from stores (after minimal normalization of field names).
    """
    ddl = """
    DROP TABLE IF EXISTS rawitems;

    CREATE TABLE rawitems (
        "Store_Name"     VARCHAR(20),
        "Store_Item_ID"  VARCHAR(50),
        "POSCode"        VARCHAR(50),
        "ItemName"       VARCHAR(255),
        "Brand"          VARCHAR(100),
        "PackSize"       VARCHAR(50),
        "Category"       VARCHAR(100)
    );
    """
    with engine.begin() as conn:
        conn.execute(text(ddl))

# create the table
create_rawitems_table(engine)

# Read raw CSVs from the three stores
store1_df = pd.read_csv("/content/Store1_Messy_Retail_Data.csv")
store2_df = pd.read_csv("/content/Store2_Messy_Retail_Data.csv")
store3_df = pd.read_csv("/content/Store3_Messy_Retail_Data.csv")

# Combine all store datasets
all_stores = pd.concat([store1_df, store2_df, store3_df], ignore_index=True)

# --- FIX: Align DataFrame columns with rawitems table schema ---

# 1) Rename StoreID -> Store_Name (this is what the table expects)
if "StoreID" in all_stores.columns:
    all_stores = all_stores.rename(columns={"StoreID": "Store_Name"})

# 2) Create Store_Item_ID.
#    Here we simply copy POSCode (as text) as the store-specific item id.
all_stores["Store_Item_ID"] = all_stores["POSCode"].astype(str)

# 3) Ensure all relevant columns exist and are strings
for col in ["Store_Name", "POSCode", "ItemName", "Brand", "PackSize", "Category", "Store_Item_ID"]:
    if col in all_stores.columns:
        all_stores[col] = all_stores[col].astype(str)
    else:
        # If any column is missing, create it with empty strings
        all_stores[col] = ""

# 4) Insert only the columns that match the table definition,
#    and in the same order.
rawitems_cols = [
    "Store_Name",
    "Store_Item_ID",
    "POSCode",
    "ItemName",
    "Brand",
    "PackSize",
    "Category",
]

all_stores[rawitems_cols].to_sql("rawitems", engine, if_exists="append", index=False)

# ============================================================
# 2. Build itemmaster_initial with cleaned attributes
# ============================================================

def build_itemmaster_initial(engine):
    """
    Create the itemmaster_initial table with a surrogate raw_item_id and
    cleaned versions of key descriptive attributes.
    """
    sql = """
    DROP TABLE IF EXISTS itemmaster_initial;

    CREATE TABLE itemmaster_initial AS
    SELECT
        "Store_Name",
        "Store_Item_ID",
        "POSCode",
        "ItemName",
        "Brand",
        "PackSize",
        "Category",
        row_number() OVER ()::int AS raw_item_id
    FROM rawitems;

    ALTER TABLE itemmaster_initial
      ADD PRIMARY KEY (raw_item_id);

    ALTER TABLE itemmaster_initial
      ADD COLUMN clean_item_name VARCHAR(200),
      ADD COLUMN clean_brand     VARCHAR(100),
      ADD COLUMN clean_pack_size VARCHAR(50),
      ADD COLUMN clean_category  VARCHAR(100);
    """
    with engine.begin() as conn:
        conn.execute(text(sql))

    # Apply SQL-based cleaning to standardize text fields
    cleaning_sql = r"""
    UPDATE itemmaster_initial
    SET
      clean_item_name = UPPER(
          regexp_replace(
              regexp_replace("ItemName", '[^A-Za-z0-9& ]', ' ', 'g'),
              '\s+',
              ' ',
              'g'
          )
      ),
      clean_brand = UPPER(
          regexp_replace(
              regexp_replace("Brand", '[^A-Za-z0-9& ]', ' ', 'g'),
              '\s+',
              ' ',
              'g'
          )
      ),
      clean_pack_size = UPPER(
          regexp_replace(
              regexp_replace("PackSize", '[^A-Za-z0-9& ]', ' ', 'g'),
              '\s+',
              ' ',
              'g'
          )
      ),
      clean_category = UPPER(
          regexp_replace(
              regexp_replace("Category", '[^A-Za-z0-9& ]', ' ', 'g'),
              '\s+',
              ' ',
              'g'
          )
      );
    """
    with engine.begin() as conn:
        conn.execute(text(cleaning_sql))

build_itemmaster_initial(engine)

# ============================================================
# 3. Cluster items and build central_item_master
# ============================================================

def cluster_and_build_item_master(engine):
    """
    Use TF-IDF and agglomerative clustering to group similar items
    across stores into clusters and then use those clusters to build
    a central item master.
    """
    df = pd.read_sql("SELECT * FROM itemmaster_initial", engine)

    raw_id_col = "raw_item_id"
    clean_name_col = "clean_item_name"
    clean_brand_col = "clean_brand"
    clean_pack_col = "clean_pack_size"
    clean_cat_col = "clean_category"

    def normalize_item_name(s: str) -> str:
        """
        Normalize item names to reduce noise in clustering.
        """
        if not isinstance(s, str):
            return ""
        s = s.upper()
        s = " ".join(s.split())
        return s

    df["normalized_name"] = df[clean_name_col].apply(normalize_item_name)

    # Combined text: normalized item name + brand + pack size
    df["combined"] = (
        df["normalized_name"].fillna("")
        + " "
        + df[clean_brand_col].fillna("")
        + " "
        + df[clean_pack_col].fillna("")
    )

    # Convert combined text to TF-IDF vectors and compute cosine similarity
    tfidf = TfidfVectorizer().fit_transform(df["combined"])
    sim_matrix = cosine_similarity(tfidf)
    dist_matrix = 1 - sim_matrix

    # Hard blocking: items with different brand OR pack size don't cluster together
    brand_arr = df[clean_brand_col].fillna("").values
    pack_arr = df[clean_pack_col].fillna("").values

    brand_diff = brand_arr[:, None] != brand_arr[None, :]
    pack_diff = pack_arr[:, None] != pack_arr[None, :]
    must_separate = brand_diff | pack_diff

    dist_matrix[must_separate] = 1.0
    np.fill_diagonal(dist_matrix, 0.0)

    # Agglomerative clustering on precomputed distance matrix
    model = AgglomerativeClustering(
        n_clusters=None,
        metric="precomputed",
        linkage="average",
        distance_threshold=0.35,
    )
    df["clusterid"] = model.fit_predict(dist_matrix) + 1

    # Derive canonical category per cluster (majority vote)
    def pick_canonical_category(group: pd.DataFrame) -> str:
        counts = group[clean_cat_col].value_counts(dropna=True)
        if len(counts) == 0:
            return "UNKNOWN"
        return counts.idxmax()

    cluster_cat = (
        df.groupby("clusterid")
        .apply(pick_canonical_category)
        .rename("cluster_category")
        .reset_index()
    )
    df = df.merge(cluster_cat, on="clusterid", how="left")
    df[clean_cat_col] = df["cluster_category"]

    # Enforce one master category per canonical brand
    df["canonical_brand"] = df[clean_brand_col]

    def pick_brand_master_category(group: pd.DataFrame) -> str:
        counts = group[clean_cat_col].value_counts(dropna=True)
        if len(counts) == 0:
            return "UNKNOWN"
        return counts.idxmax()

    brand_cat = (
        df.groupby("canonical_brand")
        .apply(pick_brand_master_category)
        .rename("brand_master_category")
        .reset_index()
    )
    df = df.merge(brand_cat, on="canonical_brand", how="left")
    df[clean_cat_col] = df["brand_master_category"]

    # Create item_master_clustered table
    with engine.begin() as conn:
        conn.execute(text("DROP TABLE IF EXISTS item_master_clustered;"))
        conn.execute(
            text(
                """
                CREATE TABLE item_master_clustered (
                    clusterid        INT,
                    raw_item_id      INT,
                    clean_item_name  VARCHAR(255),
                    canonical_brand  VARCHAR(100),
                    clean_pack_size  VARCHAR(50),
                    clean_category   VARCHAR(100)
                );
                """
            )
        )

    out = df[
        [
            "clusterid",
            raw_id_col,
            clean_name_col,
            "canonical_brand",
            clean_pack_col,
            clean_cat_col,
        ]
    ]
    out.columns = [
        "clusterid",
        "raw_item_id",
        "clean_item_name",
        "canonical_brand",
        "clean_pack_size",
        "clean_category",
    ]
    out.to_sql("item_master_clustered", engine, if_exists="append", index=False)

    # Derive central_item_master by collapsing each cluster to one master item
    clustered_sql = """
    DROP TABLE IF EXISTS central_item_master;

    CREATE TABLE central_item_master AS
    SELECT
        clusterid              AS "MasterID",
        MIN(clean_item_name)   AS "Master_Item_Name",
        MIN(canonical_brand)   AS "Master_Brand",
        MIN(clean_pack_size)   AS "Master_Pack_Size",
        MIN(clean_category)    AS "Master_Category",
        COUNT(*)               AS "RawItemsCount"
    FROM item_master_clustered
    GROUP BY clusterid;
    """
    with engine.begin() as conn:
        conn.execute(text(clustered_sql))

cluster_and_build_item_master(engine)

# ============================================================
# 4. Build review_queue and itemmapping tables
# ============================================================

def build_review_queue_and_mapping(engine):
    """
    Create the review_queue for single-item clusters and the itemmapping table
    to link raw items to their central master.
    """
    review_sql = """
    DROP TABLE IF EXISTS review_queue;

    CREATE TABLE review_queue AS
    SELECT
        ic.raw_item_id          AS "RawItemID",
        ic.clusterid            AS "SuggestedMasterID",
        0.0                     AS "ConfidenceScore",
        'Pending'::text         AS "Status",
        imi."Store_Name",
        imi."Store_Item_ID",
        imi."ItemName",
        imi."Brand",
        imi."PackSize",
        imi."Category",
        ic.clean_item_name,
        ic.canonical_brand      AS clean_brand,
        ic.clean_pack_size,
        ic.clean_category
    FROM (
        SELECT
            imc.*,
            COUNT(*) OVER (PARTITION BY imc.clusterid) AS cluster_size
        FROM item_master_clustered imc
    ) ic
    JOIN itemmaster_initial imi
      ON ic.raw_item_id = imi.raw_item_id
    WHERE ic.cluster_size = 1;
    """

    mapping_sql = """
    DROP TABLE IF EXISTS itemmapping;

    CREATE TABLE itemmapping AS
    SELECT
        ic.raw_item_id          AS "RawItemID",
        ic.clusterid            AS "MasterID",
        1.0                     AS "ConfidenceScore",

        imi."Store_Name",
        imi."Store_Item_ID",
        imi."POSCode",
        imi."ItemName",
        imi."Brand",
        imi."PackSize",
        imi."Category",

        ic.clean_item_name,
        ic.canonical_brand      AS clean_brand,
        ic.clean_pack_size,
        ic.clean_category
    FROM item_master_clustered ic
    JOIN itemmaster_initial imi
      ON ic.raw_item_id = imi.raw_item_id;
    """

    with engine.begin() as conn:
        conn.execute(text(review_sql))
        conn.execute(text(mapping_sql))

build_review_queue_and_mapping(engine)
